{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_business = pd.read_pickle('YELP//business.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_business.to_csv('business.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_business' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_business\u001b[39m.\u001b[39mhead(\u001b[39m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_business' is not defined"
     ]
    }
   ],
   "source": [
    "data_business.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_json(\u001b[39m'\u001b[39;49m\u001b[39mYELP//review.json\u001b[39;49m\u001b[39m'\u001b[39;49m, lines\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\ABANIBI\\.pyenv\\pyenv-win\\versions\\3.10.1\\lib\\site-packages\\pandas\\io\\json\\_json.py:760\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[39mif\u001b[39;00m convert_axes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m orient \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    758\u001b[0m     convert_axes \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 760\u001b[0m json_reader \u001b[39m=\u001b[39m JsonReader(\n\u001b[0;32m    761\u001b[0m     path_or_buf,\n\u001b[0;32m    762\u001b[0m     orient\u001b[39m=\u001b[39;49morient,\n\u001b[0;32m    763\u001b[0m     typ\u001b[39m=\u001b[39;49mtyp,\n\u001b[0;32m    764\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    765\u001b[0m     convert_axes\u001b[39m=\u001b[39;49mconvert_axes,\n\u001b[0;32m    766\u001b[0m     convert_dates\u001b[39m=\u001b[39;49mconvert_dates,\n\u001b[0;32m    767\u001b[0m     keep_default_dates\u001b[39m=\u001b[39;49mkeep_default_dates,\n\u001b[0;32m    768\u001b[0m     precise_float\u001b[39m=\u001b[39;49mprecise_float,\n\u001b[0;32m    769\u001b[0m     date_unit\u001b[39m=\u001b[39;49mdate_unit,\n\u001b[0;32m    770\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    771\u001b[0m     lines\u001b[39m=\u001b[39;49mlines,\n\u001b[0;32m    772\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m    773\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    774\u001b[0m     nrows\u001b[39m=\u001b[39;49mnrows,\n\u001b[0;32m    775\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    776\u001b[0m     encoding_errors\u001b[39m=\u001b[39;49mencoding_errors,\n\u001b[0;32m    777\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[0;32m    778\u001b[0m     engine\u001b[39m=\u001b[39;49mengine,\n\u001b[0;32m    779\u001b[0m )\n\u001b[0;32m    781\u001b[0m \u001b[39mif\u001b[39;00m chunksize:\n\u001b[0;32m    782\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\n",
      "File \u001b[1;32mc:\\Users\\ABANIBI\\.pyenv\\pyenv-win\\versions\\3.10.1\\lib\\site-packages\\pandas\\io\\json\\_json.py:862\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[1;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mujson\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    861\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_data_from_filepath(filepath_or_buffer)\n\u001b[1;32m--> 862\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess_data(data)\n",
      "File \u001b[1;32mc:\\Users\\ABANIBI\\.pyenv\\pyenv-win\\versions\\3.10.1\\lib\\site-packages\\pandas\\io\\json\\_json.py:874\u001b[0m, in \u001b[0;36mJsonReader._preprocess_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunksize \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnrows):\n\u001b[0;32m    873\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 874\u001b[0m         data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunksize \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnrows):\n\u001b[0;32m    876\u001b[0m     data \u001b[39m=\u001b[39m StringIO(data)\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = pd.read_json('YELP//review.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('tip.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>compliment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGNUgVwnZUey3gcPCJ76iw</td>\n",
       "      <td>3uLgwr0qeCNMjKenHJwPGQ</td>\n",
       "      <td>Avengers time with the ladies.</td>\n",
       "      <td>2012-05-18 02:17:21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id             business_id   \n",
       "0  AGNUgVwnZUey3gcPCJ76iw  3uLgwr0qeCNMjKenHJwPGQ  \\\n",
       "\n",
       "                             text                date  compliment_count  \n",
       "0  Avengers time with the ladies. 2012-05-18 02:17:21                 0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo 'YELP//review.json' ha sido dividido en 30 partes en el directorio 'ALMACEN_REVIEW'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ruta del archivo original\n",
    "archivo_original = \"YELP//review.json\"\n",
    "\n",
    "\n",
    "# Directorio de salida para las partes divididas\n",
    "directorio_salida = \"ALMACEN_REVIEW\"\n",
    "\n",
    "# Número de partes a generar\n",
    "numero_partes = 30\n",
    "\n",
    "# Tamaño aproximado de cada parte (en bytes)\n",
    "tamano_parte = os.path.getsize(archivo_original) // numero_partes\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "if not os.path.exists(directorio_salida):\n",
    "    os.makedirs(directorio_salida)\n",
    "\n",
    "# Leer el archivo original y dividirlo en partes\n",
    "with open(archivo_original, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(numero_partes):\n",
    "        # Crear el nombre del archivo para la parte actual\n",
    "        nombre_archivo = os.path.join(directorio_salida, f\"parte_{i + 1}.json\")\n",
    "\n",
    "        # Abrir el archivo de salida para la parte actual\n",
    "        with open(nombre_archivo, \"w\", encoding=\"utf-8\") as f_parte:\n",
    "            # Escribir el contenido correspondiente a la parte actual\n",
    "            contenido_parte = f.read(tamano_parte)\n",
    "            f_parte.write(contenido_parte)\n",
    "\n",
    "# Imprimir mensaje de finalización\n",
    "print(f\"El archivo '{archivo_original}' ha sido dividido en {numero_partes} partes en el directorio '{directorio_salida}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_1.json': Unterminated string starting at: line 1 column 14 (char 13)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_1.json' convertido a CSV: 'almacen_csv\\parte_1.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_2.json': Extra data: line 1 column 4 (char 3)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_2.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_2.json' convertido a CSV: 'almacen_csv\\parte_2.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_3.json': Extra data: line 1 column 6 (char 5)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_3.json': Unterminated string starting at: line 1 column 14 (char 13)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_3.json' convertido a CSV: 'almacen_csv\\parte_3.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_4.json': Extra data: line 1 column 2 (char 1)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_4.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_4.json' convertido a CSV: 'almacen_csv\\parte_4.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_5.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_5.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_5.json' convertido a CSV: 'almacen_csv\\parte_5.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_6.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_6.json': Unterminated string starting at: line 1 column 14 (char 13)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_6.json' convertido a CSV: 'almacen_csv\\parte_6.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_7.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_7.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_7.json' convertido a CSV: 'almacen_csv\\parte_7.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_8.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_8.json': Unterminated string starting at: line 1 column 39 (char 38)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_8.json' convertido a CSV: 'almacen_csv\\parte_8.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_9.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_9.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_9.json' convertido a CSV: 'almacen_csv\\parte_9.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_10.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_10.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_10.json' convertido a CSV: 'almacen_csv\\parte_10.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_11.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_11.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_11.json' convertido a CSV: 'almacen_csv\\parte_11.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_12.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_12.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_12.json' convertido a CSV: 'almacen_csv\\parte_12.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_13.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_13.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_13.json' convertido a CSV: 'almacen_csv\\parte_13.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_14.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_14.json': Unterminated string starting at: line 1 column 14 (char 13)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_14.json' convertido a CSV: 'almacen_csv\\parte_14.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_15.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_15.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_15.json' convertido a CSV: 'almacen_csv\\parte_15.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_16.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_16.json': Expecting property name enclosed in double quotes: line 1 column 125 (char 124)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_16.json' convertido a CSV: 'almacen_csv\\parte_16.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_17.json': Extra data: line 1 column 9 (char 8)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_17.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_17.json' convertido a CSV: 'almacen_csv\\parte_17.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_18.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_18.json': Expecting ',' delimiter: line 1 column 38 (char 37)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_18.json' convertido a CSV: 'almacen_csv\\parte_18.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_19.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_19.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_19.json' convertido a CSV: 'almacen_csv\\parte_19.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_20.json': Expecting value: line 1 column 1 (char 0)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_20.json' convertido a CSV: 'almacen_csv\\parte_20.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_21.json': Unterminated string starting at: line 1 column 2 (char 1)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_21.json' convertido a CSV: 'almacen_csv\\parte_21.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_22.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_22.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_22.json' convertido a CSV: 'almacen_csv\\parte_22.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_23.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_23.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_23.json' convertido a CSV: 'almacen_csv\\parte_23.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_24.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_24.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_24.json' convertido a CSV: 'almacen_csv\\parte_24.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_25.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_25.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_25.json' convertido a CSV: 'almacen_csv\\parte_25.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_26.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_26.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_26.json' convertido a CSV: 'almacen_csv\\parte_26.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_27.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_27.json': Unterminated string starting at: line 1 column 162 (char 161)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_27.json' convertido a CSV: 'almacen_csv\\parte_27.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_28.json': Expecting value: line 1 column 1 (char 0)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_28.json': Expecting property name enclosed in double quotes: line 1 column 1078 (char 1077)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_28.json' convertido a CSV: 'almacen_csv\\parte_28.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_29.json': Extra data: line 1 column 7 (char 6)\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_29.json': Unterminated string starting at: line 1 column 14 (char 13)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_29.json' convertido a CSV: 'almacen_csv\\parte_29.csv'\n",
      "Error al cargar línea JSON en archivo 'ALMACEN_REVIEW\\parte_30.json': Expecting value: line 1 column 1 (char 0)\n",
      "Archivo 'ALMACEN_REVIEW\\parte_30.json' convertido a CSV: 'almacen_csv\\parte_30.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Directorio de entrada de las partes divididas\n",
    "directorio_entrada = \"ALMACEN_REVIEW\"\n",
    "\n",
    "# Directorio de salida para los archivos CSV\n",
    "directorio_salida = \"almacen_csv\"\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "if not os.path.exists(directorio_salida):\n",
    "    os.makedirs(directorio_salida)\n",
    "\n",
    "# Recorrer todas las partes divididas\n",
    "for i in range(1, 31):\n",
    "    # Nombre del archivo de entrada\n",
    "    nombre_archivo_entrada = os.path.join(directorio_entrada, f\"parte_{i}.json\")\n",
    "    \n",
    "    # Nombre del archivo de salida CSV\n",
    "    nombre_archivo_salida = os.path.join(directorio_salida, f\"parte_{i}.csv\")\n",
    "    \n",
    "    # Leer el archivo JSON de entrada\n",
    "    with open(nombre_archivo_entrada, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = []\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                data.append(obj)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error al cargar línea JSON en archivo '{nombre_archivo_entrada}': {e}\")\n",
    "    \n",
    "    # Obtener las claves únicas de los objetos JSON\n",
    "    keys = set()\n",
    "    for item in data:\n",
    "        keys.update(item.keys())\n",
    "    \n",
    "    # Escribir el archivo CSV de salida\n",
    "    with open(nombre_archivo_salida, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(keys))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "    \n",
    "    print(f\"Archivo '{nombre_archivo_entrada}' convertido a CSV: '{nombre_archivo_salida}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "parte_01 = pd.read_csv(\"almacen_csv\\parte_1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cool', 'text', 'funny', 'business_id', 'date', 'review_id', 'useful',\n",
       "       'user_id', 'stars'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parte_01.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "parte_02 = pd.read_csv(\"almacen_csv\\parte_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cool', 'text', 'funny', 'business_id', 'date', 'review_id', 'useful',\n",
       "       'user_id', 'stars'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parte_02.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m# Leer el archivo CSV\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(nombre_archivo, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> 17\u001b[0m     reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mDictReader(f)\n\u001b[0;32m     18\u001b[0m     \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m reader:\n\u001b[0;32m     19\u001b[0m         registros\u001b[39m.\u001b[39mappend(row)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Directorio de entrada de los archivos CSV\n",
    "directorio_entrada = \"almacen_csv\"\n",
    "\n",
    "# Nombre del archivo CSV de salida\n",
    "archivo_salida = \"review.csv\"\n",
    "\n",
    "# Lista para almacenar todos los registros\n",
    "registros = []\n",
    "\n",
    "# Recorrer todos los archivos CSV\n",
    "for i in range(1, 31):\n",
    "    # Nombre del archivo de entrada\n",
    "    nombre_archivo = os.path.join(directorio_entrada, f\"parte_{i}.csv\")\n",
    "    \n",
    "    # Leer el archivo CSV\n",
    "    with open(nombre_archivo, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            registros.append(row)\n",
    "\n",
    "# Obtener las claves únicas de todos los registros\n",
    "claves = set()\n",
    "for row in registros:\n",
    "    claves.update(row.keys())\n",
    "\n",
    "# Escribir el archivo CSV de salida\n",
    "with open(archivo_salida, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=list(claves))\n",
    "    writer.writeheader()\n",
    "    writer.writerows(registros)\n",
    "\n",
    "print(f\"Archivos CSV unidos en '{archivo_salida}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡La conversión de Parquet a CSV se ha completado correctamente!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Ruta del archivo Parquet de entrada\n",
    "archivo_parquet = 'YELP//user.parquet'\n",
    "\n",
    "# Ruta del archivo CSV de salida\n",
    "archivo_csv = 'user_01.csv'\n",
    "\n",
    "# Leer el archivo Parquet en un DataFrame de pandas\n",
    "df = pd.read_parquet(archivo_parquet)\n",
    "\n",
    "# Guardar el DataFrame en formato CSV\n",
    "df.to_csv(\"user.csv\", index=False)\n",
    "\n",
    "print(\"¡La conversión de Parquet a CSV se ha completado correctamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABANIBI\\AppData\\Local\\Temp\\ipykernel_7508\\2828289432.py:1: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_user= pd.read_csv(\"user.csv\")\n"
     ]
    }
   ],
   "source": [
    "data_user= pd.read_csv(\"user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>review_count</th>\n",
       "      <th>yelping_since</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>elite</th>\n",
       "      <th>friends</th>\n",
       "      <th>fans</th>\n",
       "      <th>...</th>\n",
       "      <th>compliment_more</th>\n",
       "      <th>compliment_profile</th>\n",
       "      <th>compliment_cute</th>\n",
       "      <th>compliment_list</th>\n",
       "      <th>compliment_note</th>\n",
       "      <th>compliment_plain</th>\n",
       "      <th>compliment_cool</th>\n",
       "      <th>compliment_funny</th>\n",
       "      <th>compliment_writer</th>\n",
       "      <th>compliment_photos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qVc8ODYU5SZjKXVBgXdI7w</td>\n",
       "      <td>Walker</td>\n",
       "      <td>585</td>\n",
       "      <td>2007-01-25 16:47:26</td>\n",
       "      <td>7217</td>\n",
       "      <td>1259</td>\n",
       "      <td>5994</td>\n",
       "      <td>2007</td>\n",
       "      <td>NSCy54eWehBJyZdG2iE84w, pe42u7DcCH2QmI81NX-8qA...</td>\n",
       "      <td>267</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>18</td>\n",
       "      <td>232</td>\n",
       "      <td>844</td>\n",
       "      <td>467</td>\n",
       "      <td>467</td>\n",
       "      <td>239</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>j14WgRoU_-2ZE1aw1dXrJg</td>\n",
       "      <td>Daniel</td>\n",
       "      <td>4333</td>\n",
       "      <td>2009-01-25 04:35:42</td>\n",
       "      <td>43091</td>\n",
       "      <td>13066</td>\n",
       "      <td>27281</td>\n",
       "      <td>2009,2010,2011,2012,2013,2014,2015,2016,2017,2...</td>\n",
       "      <td>ueRPE0CX75ePGMqOFVj6IQ, 52oH4DrRvzzl8wh5UXyU0A...</td>\n",
       "      <td>3138</td>\n",
       "      <td>...</td>\n",
       "      <td>264</td>\n",
       "      <td>184</td>\n",
       "      <td>157</td>\n",
       "      <td>251</td>\n",
       "      <td>1847</td>\n",
       "      <td>7054</td>\n",
       "      <td>3131</td>\n",
       "      <td>3131</td>\n",
       "      <td>1521</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2WnXYQFK0hXEoTxPtV2zvg</td>\n",
       "      <td>Steph</td>\n",
       "      <td>665</td>\n",
       "      <td>2008-07-25 10:41:00</td>\n",
       "      <td>2086</td>\n",
       "      <td>1010</td>\n",
       "      <td>1003</td>\n",
       "      <td>2009,2010,2011,2012,2013</td>\n",
       "      <td>LuO3Bn4f3rlhyHIaNfTlnA, j9B4XdHUhDfTKVecyWQgyA...</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>96</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>35</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SZDeASXq7o05mMNLshsdIA</td>\n",
       "      <td>Gwen</td>\n",
       "      <td>224</td>\n",
       "      <td>2005-11-29 04:38:33</td>\n",
       "      <td>512</td>\n",
       "      <td>330</td>\n",
       "      <td>299</td>\n",
       "      <td>2009,2010,2011</td>\n",
       "      <td>enx1vVPnfdNUdPho6PH_wg, 4wOcvMLtU6a9Lslggq74Vg...</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hA5lMy-EnncsH4JoR-hFGQ</td>\n",
       "      <td>Karen</td>\n",
       "      <td>79</td>\n",
       "      <td>2007-01-05 19:40:59</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PBK4q9KEEBHhFvSXCUirIw, 3FWPpM7KU1gXeOM_ZbYMbA...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id    name  review_count        yelping_since  useful   \n",
       "0  qVc8ODYU5SZjKXVBgXdI7w  Walker           585  2007-01-25 16:47:26    7217  \\\n",
       "1  j14WgRoU_-2ZE1aw1dXrJg  Daniel          4333  2009-01-25 04:35:42   43091   \n",
       "2  2WnXYQFK0hXEoTxPtV2zvg   Steph           665  2008-07-25 10:41:00    2086   \n",
       "3  SZDeASXq7o05mMNLshsdIA    Gwen           224  2005-11-29 04:38:33     512   \n",
       "4  hA5lMy-EnncsH4JoR-hFGQ   Karen            79  2007-01-05 19:40:59      29   \n",
       "\n",
       "   funny   cool                                              elite   \n",
       "0   1259   5994                                               2007  \\\n",
       "1  13066  27281  2009,2010,2011,2012,2013,2014,2015,2016,2017,2...   \n",
       "2   1010   1003                           2009,2010,2011,2012,2013   \n",
       "3    330    299                                     2009,2010,2011   \n",
       "4     15      7                                                NaN   \n",
       "\n",
       "                                             friends  fans  ...   \n",
       "0  NSCy54eWehBJyZdG2iE84w, pe42u7DcCH2QmI81NX-8qA...   267  ...  \\\n",
       "1  ueRPE0CX75ePGMqOFVj6IQ, 52oH4DrRvzzl8wh5UXyU0A...  3138  ...   \n",
       "2  LuO3Bn4f3rlhyHIaNfTlnA, j9B4XdHUhDfTKVecyWQgyA...    52  ...   \n",
       "3  enx1vVPnfdNUdPho6PH_wg, 4wOcvMLtU6a9Lslggq74Vg...    28  ...   \n",
       "4  PBK4q9KEEBHhFvSXCUirIw, 3FWPpM7KU1gXeOM_ZbYMbA...     1  ...   \n",
       "\n",
       "   compliment_more  compliment_profile  compliment_cute  compliment_list   \n",
       "0               65                  55               56               18  \\\n",
       "1              264                 184              157              251   \n",
       "2               13                  10               17                3   \n",
       "3                4                   1                6                2   \n",
       "4                1                   0                0                0   \n",
       "\n",
       "   compliment_note  compliment_plain  compliment_cool  compliment_funny   \n",
       "0              232               844              467               467  \\\n",
       "1             1847              7054             3131              3131   \n",
       "2               66                96              119               119   \n",
       "3               12                16               26                26   \n",
       "4                1                 1                0                 0   \n",
       "\n",
       "   compliment_writer  compliment_photos  \n",
       "0                239                180  \n",
       "1               1521               1946  \n",
       "2                 35                 18  \n",
       "3                 10                  9  \n",
       "4                  0                  0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_user.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "division csv para diseñar ER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "divide el archivo csv con 100 filas \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo CSV se ha dividido en partes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def split_csv(input_file, output_directory, chunk_size):\n",
    "    # Crea el directorio de salida si no existe\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Lee el archivo CSV utilizando pandas\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Obtiene los encabezados del archivo CSV\n",
    "    headers = list(df.columns)\n",
    "\n",
    "    # Divide el DataFrame en partes más pequeñas\n",
    "    chunks = [df[i:i+chunk_size] for i in range(0, df.shape[0], chunk_size)]\n",
    "\n",
    "    # Guarda cada parte en un archivo CSV separado\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        output_file = os.path.join(output_directory, f'part_{i+1}.csv')\n",
    "        chunk.to_csv(output_file, index=False, header=headers)\n",
    "\n",
    "    print('El archivo CSV se ha dividido en partes.')\n",
    "\n",
    "# Ejemplo de uso\n",
    "input_file = 'YELP_CSV//tip.csv'  # Nombre del archivo CSV de entrada\n",
    "output_directory = 'entidad_relacion'  # Directorio de salida para las partes divididas\n",
    "chunk_size = 1000  # Tamaño máximo de cada parte (número de filas)\n",
    "\n",
    "split_csv(input_file, output_directory, chunk_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "divide el archivo csv en 30 partes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def split_csv(input_file, output_directory):\n",
    "    # Crea el directorio de salida si no existe\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Abre el archivo CSV de entrada\n",
    "    with open(input_file, 'r') as file:\n",
    "        # Lee el archivo CSV utilizando el módulo csv\n",
    "        reader = csv.reader(file)\n",
    "        # Obtiene los encabezados del archivo CSV\n",
    "        headers = next(reader)\n",
    "\n",
    "        # Calcula el tamaño de cada parte\n",
    "        total_rows = sum(1 for _ in reader)\n",
    "        file.seek(0)  # Vuelve al inicio del archivo\n",
    "        chunk_size = total_rows // 30 + (1 if total_rows % 30 != 0 else 0)\n",
    "\n",
    "        # Inicializa las variables para el seguimiento de los archivos de salida\n",
    "        current_chunk = 1\n",
    "        current_rows = []\n",
    "        current_rows_count = 0\n",
    "\n",
    "        # Lee cada fila del archivo CSV\n",
    "        for row in reader:\n",
    "            # Agrega la fila actual a la lista de filas del archivo de salida actual\n",
    "            current_rows.append(row)\n",
    "            current_rows_count += 1\n",
    "\n",
    "            # Verifica si se ha alcanzado el tamaño máximo de la parte\n",
    "            if current_rows_count >= chunk_size:\n",
    "                # Genera el nombre del archivo de salida para la parte actual\n",
    "                output_file = os.path.join(output_directory, f'part_{current_chunk}.csv')\n",
    "\n",
    "                # Escribe las filas en el archivo de salida actual\n",
    "                with open(output_file, 'w', newline='') as output:\n",
    "                    writer = csv.writer(output)\n",
    "                    writer.writerow(headers)\n",
    "                    writer.writerows(current_rows)\n",
    "\n",
    "                # Reinicia las variables para la siguiente parte\n",
    "                current_chunk += 1\n",
    "                current_rows = []\n",
    "                current_rows_count = 0\n",
    "\n",
    "        # Escribe las filas restantes en el último archivo de salida si hay alguna\n",
    "        if current_rows:\n",
    "            output_file = os.path.join(output_directory, f'part_{current_chunk}.csv')\n",
    "            with open(output_file, 'w', newline='') as output:\n",
    "                writer = csv.writer(output)\n",
    "                writer.writerow(headers)\n",
    "                writer.writerows(current_rows)\n",
    "\n",
    "        print('El archivo CSV se ha dividido en 30 partes.')\n",
    "\n",
    "# Ejemplo de uso\n",
    "input_file = 'archivo.csv'  # Nombre del archivo CSV de entrada\n",
    "output_directory = 'partes_csv'  # Directorio de salida para las partes divididas\n",
    "\n",
    "split_csv(input_file, output_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
